---
title: "Short Course on R Tools"
subtitle: "Deep Learning with R and Keras"
title-slide-attributes:
  data-background-image: mu-bg.png
  data-background-size: stretch
  data-slide-number: none
format: revealjs
---

# Outline

1.  Introduction & Setup

2.  Fundamentals of Neural Networks

3.  Building MLPs for Classification & Regression

4.  Convolutional Neural Networks (CNNs)

5.  Recurrent Neural Networks (RNNs) & LSTM

6.  Autoencoders & Unsupervised Learning

7.  Model Evaluation & Tuning

8.  Advanced Topics & Deployment

9.  Hands-on Exercises

# 1. Introduction & Environment Setup

-   **Why Deep Learning?** complex function approximation, feature engineering

-   **Keras in R**: high-level API for TensorFlow backend

-   **Installation**:

    ``` {.r}
    install.packages("keras")
    library(keras)
    install_keras()  # installs TensorFlow
    ```

-   **Project structure**: data/, scripts/, models/

??? note Demo environment check: `tensorflow::tf_config()`.

# 2. Fundamentals of Neural Networks

-   **Perceptron**: basic neuron → activation(input \* weight + bias)

-   **Layers**: Dense layers, activation functions (`relu`, `sigmoid`, `softmax`)

-   **Forward & backward pass**: feedforward, gradient descent

::: {.shrink-code}
``` {.r}
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 10, activation = 'softmax')
```
:::

??? note Visualize network architecture with `plot_model()`.

# 3. Multilayer Perceptrons (MLPs)

## Classification on MNIST

``` {.r}
model %>% compile(
  optimizer = 'rmsprop',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)
history <- model %>% fit(
  x_train, y_train,
  epochs = 10, batch_size = 128,
  validation_split = 0.2
)
```

-   **Data preprocessing**: normalization, one-hot encoding

-   **Callbacks**: EarlyStopping, ModelCheckpoint

??? note Show training curves: `plot(history)`.

# 4. Convolutional Neural Networks (CNNs)

-   **Concepts**: filters, pooling, feature maps

-   **Example**:

``` {.r}
cnn <- keras_model_sequential() %>%
  layer_conv_2d(filters=32, kernel_size=c(3,3), activation='relu', input_shape=c(28,28,1)) %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), activation='relu') %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_flatten() %>%
  layer_dense(units=64, activation='relu') %>%
  layer_dense(units=10, activation='softmax')
```

-   **Image augmentation**: `image_data_generator()`

??? note Demo augmenting and fitting on CIFAR-10 subset.

# 5. Recurrent Neural Networks & LSTM

-   **RNN basics**: sequence data, time steps

-   **LSTM**: handling long-term dependencies

``` {.r}
rnn <- keras_model_sequential() %>%
  layer_lstm(units=128, input_shape=c(timesteps, features)) %>%
  layer_dense(units=1, activation='sigmoid')
```

-   **Use case**: sentiment analysis, text generation

??? note Show example on IMDB dataset: `dataset_imdb()`.

# 6. Autoencoders & Unsupervised Learning

-   **Architecture**: encoder ⇄ bottleneck ⇄ decoder

``` {.r}
ae <- keras_model_sequential() %>%
  layer_dense(units=64, activation='relu', input_shape=c(784)) %>%
  layer_dense(units=32, activation='relu') %>%
  layer_dense(units=64, activation='relu') %>%
  layer_dense(units=784, activation='sigmoid')
```

-   **Applications**: dimensionality reduction, denoising

??? note Demo training on noisy MNIST.

# 7. Model Evaluation & Hyperparameter Tuning

-   **Metrics**: accuracy, precision/recall, ROC-AUC

-   **Plots**: confusion matrix, learning curves

-   **Tuning**: `tfruns`, `keras_tuner`

-   **K-fold CV**: manual splits or `rsample`

??? note Illustrate hyperband tuning example.

# 8. Advanced Topics & Deployment

-   **Transfer Learning**: `application_resnet50()`, fine-tuning

-   **Custom layers & callbacks**

-   **Model saving/loading**: `save_model_hdf5()`, `load_model_hdf5()`

-   **Deployment**: Plumber API, TensorFlow Serving, Shiny integration

??? note Example: wrap model in Plumber endpoint.

# 9. Hands-on Exercises (30 min)

1.  Build and train a CNN on Fashion MNIST

2.  Implement an LSTM for sequence prediction (e.g., sine wave)

3.  Create an autoencoder for dimensionality reduction and visualize embeddings

??? note Encourage pair work and code sharing.

# Resources & Further Reading

-   Book: *Deep Learning with R* by Francois Chollet & J.J. Allaire

-   Keras docs: https://keras.rstudio.com/

-   TensorFlow for R: https://tensorflow.rstudio.com/

-   Tutorials: https://www.coursera.org/learn/deep-learning

??? note Provide links for reference.
