---
title: "Short Course on R Tools"
subtitle: "Deep Learning in R"
title-slide-attributes:
  data-background-image: mu-bg.png
  data-background-size: stretch
  data-slide-number: none
format:
  revealjs:
    transition: fade
    scrollable: true
    revealjs-plugins:
     - fullscreen
---

# Outline

::: {.fragment .fade-up}
-   Introduction & Setup
-   Fundamentals of Neural Networks
-   Building MLPs for Classification & Regression
-   Convolutional Neural Networks (CNNs)
-   Recurrent Neural Networks (RNNs) & LSTM
-   Autoencoders & Unsupervised Learning
-   Model Evaluation & Tuning
-   Advanced Topics & Deployment
-   Hands-on Exercises
:::

# 1. Introduction & Environment Setup

-   **Why Deep Learning?** complex function approximation, feature engineering

-   **Keras in R**: high-level API for TensorFlow backend

-   **Installation**:

    ``` r
    install.packages("keras")
    library(keras)
    install_keras()  # installs TensorFlow
    ```

-   **Project structure**: data/, scripts/, models/

??? note Demo environment check: `tensorflow::tf_config()`.

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

-   Deep learning is often presented as algorithms that ‚Äúwork like the brain‚Äù, that ‚Äúthink‚Äù or ‚Äúunderstand‚Äù.

. . . 

<div style="margin-top: -30px;"><center>[Reality is however quite far from this dream]{.text-red}</center></div>

. . .

-   AI: the effort to automate intellectual tasks normally performed by humans.

![](images/DLiR/classical-programming.png){.absolute top=210 left=80 width="400px"} 
<div style="margin-top: 90px;"></div>

. . .

-   ML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data?
![](images/DLiR/machine-learning.png){.absolute top=225 left=530 width="400px"} 
<div style="margin-top: -40px;"></div>

. . .

:::::: {.r-hstack style="margin-top: 1em;"}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 200px; height: 150px; margin: 10px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Artificial Intelligence
:::

::: {data-id="box2" auto-animate-delay="0.1" style="background: #3fb618; width: 200px; height: 150px; margin: 10px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Machine learning
:::

::: {data-id="box3" auto-animate-delay="0.2" style="background: #e83e8c; width: 200px; height: 150px; margin: 10px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Deep learning
:::
::::::

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

-   Deep learning is often presented as algorithms that ‚Äúwork like the brain‚Äù, that ‚Äúthink‚Äù or ‚Äúunderstand‚Äù.

<div style="margin-top: -30px;">[&nbsp;]{.text-red}</center></div>

-   AI: the effort to automate intellectual tasks normally performed by humans.

![](images/DLiR/classical-programming.png){.absolute top=210 left=80 width="400px"} 
<div style="margin-top: 90px;"></div>

-   ML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data?
![](images/DLiR/machine-learning.png){.absolute top=225 left=530 width="400px"} 
<div style="margin-top: -40px;"></div>

::::::: r-stack
:::::: {.r-stack style="margin-top: 1em;"}
::: {data-id="box1" style="background: #2780e3; width: 600px; height: 200px; border-radius: 200px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Artificial Intelligence
:::

::: {data-id="box2" style="background: #3fb618; width: 430px; height: 130px; border-radius: 200px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Machine learning
:::

::: {data-id="box3" style="background: #e83e8c; width: 260px; height: 60px; border-radius: 200px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Deep learning
:::
::::::

![](images/DLiR/AI-ML-DL.png){.fragment width="750"}
:::::::

## Recipes of a Machine Learning Algorithm {.smaller auto-animate="true"}
::: incremental
-   Input data points, e.g.
![](images/DLiR/mse-1.png){.absolute bottom=0 left=300 width="400"}
    -   If the task is speech recognition, these data points could be sound files
    -   If the task is image tagging, they could be picture files

-   Examples of the expected output
![](images/DLiR/mse-2.png){.absolute bottom=0 left=300 width="400"}
    -   In a speech-recognition task, these could be transcripts of sound files
    -   In an image task, expected outputs could tags such as "dog", "cat", and so on

-   A way to measure whether the algorithm is doing a good job
![](images/DLiR/mse-3.png){.absolute bottom=0 left=300 width="400"}
    -   This is needed to determine the distance between the output and its expected output.
    -   The measurement is used as a feedback signal to adjust the way the algorithm works.
<div style="margin-top: -30px;">![](images/DLiR/mse-4.png){.absolute bottom=0 left=300 width="400"}</div>

:::

## Anatomy of a Neural Network {auto-animate="true"}
<div style="margin-top: -20px;"></div>
::: incremental
-   The [input data]{.text-red} and corresponding [targets]{.text-red}
![](images/DLiR/nn-1.png){.absolute bottom=0 left=200 width=500}

-   [Layers]{.text-red}, which are combined into a [network (or model)]{.text-red}
![](images/DLiR/nn-2.png){.absolute bottom=0 left=200 width=500}

-   The [loss function]{.text-red}, which provides feedback for learning
![](images/DLiR/nn-3.png){.absolute bottom=0 left=200 width=500}

-   The [optimizer]{.text-red}, which determines how learning proceeds
![](images/DLiR/nn-4.png){.absolute bottom=0 left=200 width=500}
:::

## LeNet-5: a pioneering 7-level CNN {auto-animate="true" .smaller}
[<img src="images/DLiR/LeNet-5_architecture.png" style="opacity: 0.10;">]{.absolute top=75 left=0}

::: incremental
-   The first successful practical application of neural nets came in 1989 from Bell Labs, when [Yann LeCun]{.text-red} combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits.

-   The resulting network, dubbed LeNet, was used by the USPS in the 1990s to automate the reading of ZIP codes on mail envelopes.

-   LeNet-5 was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.
:::

<center>![](images/DLiR/cnn.gif){width=550}</center>

## Why 30+ Years gap? {auto-animate="true"}
[<img src="images/DLiR/imagenet-1.png" style="opacity: 0.10" width=1200>]{.absolute top=75 left=0}

::: incremental
-   In 2011, Dan Ciresan from IDSIA (Switzerland) began to win academic image-classification competitions with GPU-trained deep neural networks

-   in 2012, a team led by Alex Krizhevsky and advised by [Geoffrey Hinton]{.text-red} was able to achieve a top-five accuracy of 83.6%--a significant breakthrough (in 2011 it was only 74.3%).
![](images/DLiR/imagenet-2.png){.absolute bottom=0 right=0 width=350 height=300}

-   Three forces are driving advances in ML:
    -   Hardware
    -   Datasets and benchmarks
    -   Algorithmic advances
:::

## VGG16‚ÄìCNN for Classification and Detection {auto-animate="true" .smaller}
[<img src="images/DLiR/VGG16-1.png" style="opacity: 0.10" width="1500">]{.absolute top=0 right=0}

::: incremental
-   VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford.

-   The model achieves 92.7% top-5 test accuracy in ImageNet. It was one of the famous model submitted to ILSVRC-2014.

-   It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3√ó3 kernel-sized filters one after another.

-   VGG16 was trained for weeks using NVIDIA Titan Black GPU‚Äôs.
![](images/DLiR/VGG16-2.png){.absolute bottom=0 left=250 width=500}
:::

# 2. Fundamentals of Neural Networks

-   **Perceptron**: basic neuron ‚Üí activation(input \* weight + bias)

-   **Layers**: Dense layers, activation functions (`relu`, `sigmoid`, `softmax`)

-   **Forward & backward pass**: feedforward, gradient descent

::: shrink-code
``` r
model <- keras_model_sequential(input_shape = c(784)) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')
```
:::

??? note Visualize network architecture with `plot_model()`.

[üîó Launch R](http://tinyurl.com/dna-Rstudio){target=_blank}

## Neural Network ‚Äì Parameters - Activation Func. {.r-fit-text}

![](images/DLiR/nn-01.png){.absolute bottom=0 left=0 width=500}

. . .

<div style="margin-top: 55px;"></div>
<span style="padding-left: 3.5em;">**A Neural Network**</span>

. . .

![](images/DLiR/nn-02.png){.absolute bottom=0 left=0 width=500}

. . . 

![](images/DLiR/nn-03.png){.absolute bottom=0 left=0 width=500}

. . .

![](images/DLiR/af-1.png){.absolute bottom=100 right=0 width=500}

. . . 

<div style="margin-top: -55px;"></div>
<span style="padding-left: 17.5em;">**Activation Function**</span>

. . .

![](images/DLiR/af-2.png){.absolute bottom=100 right=0 width=500}

# 3. Multilayer Perceptrons (MLPs)

## Classification on MNIST

``` r
model %>% compile(
  optimizer = 'rmsprop',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)
history <- model %>% fit(
  x_train, y_train,
  epochs = 10, batch_size = 128,
  validation_split = 0.2
)
```

-   **Data preprocessing**: normalization, one-hot encoding

-   **Callbacks**: EarlyStopping, ModelCheckpoint

??? note Show training curves: `plot(history)`.

# 4. Convolutional Neural Networks (CNNs)

-   **Concepts**: filters, pooling, feature maps

-   **Example**:

``` r
cnn <- keras_model_sequential() %>%
  layer_conv_2d(filters=32, kernel_size=c(3,3), activation='relu', input_shape=c(28,28,1)) %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), activation='relu') %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_flatten() %>%
  layer_dense(units=64, activation='relu') %>%
  layer_dense(units=10, activation='softmax')
```

-   **Image augmentation**: `image_data_generator()`

??? note Demo augmenting and fitting on CIFAR-10 subset.

# 5. Recurrent Neural Networks & LSTM

-   **RNN basics**: sequence data, time steps

-   **LSTM**: handling long-term dependencies

``` r
rnn <- keras_model_sequential() %>%
  layer_lstm(units=128, input_shape=c(timesteps, features)) %>%
  layer_dense(units=1, activation='sigmoid')
```

-   **Use case**: sentiment analysis, text generation

??? note Show example on IMDB dataset: `dataset_imdb()`.

# 6. Autoencoders & Unsupervised Learning

-   **Architecture**: encoder ‚áÑ bottleneck ‚áÑ decoder

``` r
ae <- keras_model_sequential() %>%
  layer_dense(units=64, activation='relu', input_shape=c(784)) %>%
  layer_dense(units=32, activation='relu') %>%
  layer_dense(units=64, activation='relu') %>%
  layer_dense(units=784, activation='sigmoid')
```

-   **Applications**: dimensionality reduction, denoising

??? note Demo training on noisy MNIST.

# 7. Model Evaluation & Hyperparameter Tuning

-   **Metrics**: accuracy, precision/recall, ROC-AUC

-   **Plots**: confusion matrix, learning curves

-   **Tuning**: `tfruns`, `keras_tuner`

-   **K-fold CV**: manual splits or `rsample`

??? note Illustrate hyperband tuning example.

# 8. Advanced Topics & Deployment

-   **Transfer Learning**: `application_resnet50()`, fine-tuning

-   **Custom layers & callbacks**

-   **Model saving/loading**: `save_model_hdf5()`, `load_model_hdf5()`

-   **Deployment**: Plumber API, TensorFlow Serving, Shiny integration

??? note Example: wrap model in Plumber endpoint.

# 9. Hands-on Exercises (30 min)

1.  Build and train a CNN on Fashion MNIST

2.  Implement an LSTM for sequence prediction (e.g., sine wave)

3.  Create an autoencoder for dimensionality reduction and visualize embeddings

??? note Encourage pair work and code sharing.

# Resources & Further Reading

-   Book: *Deep Learning with R* by Francois Chollet & J.J. Allaire

-   Keras docs: https://keras.rstudio.com/

-   TensorFlow for R: https://tensorflow.rstudio.com/

-   Tutorials: https://www.coursera.org/learn/deep-learning

??? note Provide links for reference.
