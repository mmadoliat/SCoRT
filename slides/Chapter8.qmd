---
title: "Short Course on R Tools"
subtitle: "Deep Learning in R"
title-slide-attributes:
  data-background-image: mu-bg.png
  data-background-size: stretch
  data-slide-number: none
format:
  revealjs:
    transition: fade
    scrollable: true
    revealjs-plugins:
     - fullscreen
---

# Outline

::: {.fragment .fade-up}
-   Introduction & Setup
-   Fundamentals of Neural Networks
-   Building MLPs for Classification & Regression
-   Convolutional Neural Networks (CNNs)
-   Recurrent Neural Networks (RNNs) & LSTM
-   Autoencoders & Unsupervised Learning
-   Model Evaluation & Tuning
-   Advanced Topics & Deployment
-   Hands-on Exercises
:::

# 1. Introduction & Environment Setup

-   **Why Deep Learning?** complex function approximation, feature engineering

-   **Keras in R**: high-level API for TensorFlow backend

-   **Installation**:

    ``` r
    install.packages("keras")
    library(keras)
    install_keras()  # installs TensorFlow
    ```

-   **Project structure**: data/, scripts/, models/

??? note Demo environment check: `tensorflow::tf_config()`.

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

- Deep learning is often presented as algorithms that “work like the brain”, that “think” or “understand”.

::: {.fragment .fade-in-then-out}
<center>[Reality is however quite far from this dream]{.text-red}</center>
:::

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

- Deep learning is often presented as algorithms that “work like the brain”, that “think” or “understand”.
- AI: the effort to automate intellectual tasks normally performed by humans.

::: r-stack
![](images/DLiR/classical-programming.png){width="400px"}
:::

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

- Deep learning is often presented as algorithms that “work like the brain”, that “think” or “understand”.
- AI: the effort to automate intellectual tasks normally performed by humans.
- ML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data?

::: r-stack
![](images/DLiR/classical-programming.png){width="400px"} ![](images/DLiR/machine-learning.png){width="400px"}
:::

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

- Deep learning is often presented as algorithms that “work like the brain”, that “think” or “understand”.
- AI: the effort to automate intellectual tasks normally performed by humans.
- ML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data?

::: r-stack
![](images/DLiR/classical-programming.png){width="400px"} ![](images/DLiR/machine-learning.png){width="400px"}
:::

::: {.r-hstack style="margin-top: 1em;"}
::: {data-id="box1" auto-animate-delay="0" style="background: #2780e3; width: 200px; height: 150px; margin: 10px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Artificial Intelligence
:::

::: {data-id="box2" auto-animate-delay="0.1" style="background: #3fb618; width: 200px; height: 150px; margin: 10px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Machine learning
:::

::: {data-id="box3" auto-animate-delay="0.2" style="background: #e83e8c; width: 200px; height: 150px; margin: 10px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Deep learning
:::
:::

## Classical Programming vs Machine Learning {.smaller auto-animate="true"}

- Deep learning is often presented as algorithms that “work like the brain”, that “think” or “understand”.
- AI: the effort to automate intellectual tasks normally performed by humans.
- ML: Could a computer surprise us? Rather than programmers crafting data-processing rules by hand, could a computer automatically learn these rules by looking at data?

::: r-stack
![](images/DLiR/classical-programming.png){width="400px"} ![](images/DLiR/machine-learning.png){width="400px"}
:::

::: r-stack
::: {.r-stack style="margin-top: 1em;"}
::: {data-id="box1" style="background: #2780e3; width: 600px; height: 200px; border-radius: 200px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Artificial Intelligence
:::

::: {data-id="box2" style="background: #3fb618; width: 430px; height: 130px; border-radius: 200px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Machine learning
:::

::: {data-id="box3" style="background: #e83e8c; width: 260px; height: 60px; border-radius: 200px; display: flex; align-items: top; justify-content: center; color: white; font-size: 0.5em;"}
Deep learning
:::
:::

![](images/DLiR/AI-ML-DL.png){.fragment width="750"}
:::

## Recipes of a Machine Learning Algorithm {.smaller auto-animate="true"}
- Input data points, e.g.
  - If the task is speech recognition, these data points could be sound files
  - If the task is image tagging, they could be picture files

::: r-stack
![](images/DLiR/mse-1.png){.fragment width="450"}
:::

## Recipes of a Machine Learning Algorithm {.smaller auto-animate="true"}
- Input data points, e.g.
  - If the task is speech recognition, these data points could be sound files
  - If the task is image tagging, they could be picture files
- Examples of the expected output
  - In a speech-recognition task, these could be transcripts of sound files
  - In an image task, expected outputs could tags such as "dog", "cat", and so on

::: r-stack
![](images/DLiR/mse-1.png){.fragment width="450"}

![](images/DLiR/mse-2.png){.fragment width="450"}
:::

## Recipes of a Machine Learning Algorithm {.smaller auto-animate="true"}
- Input data points, e.g.
  - If the task is speech recognition, these data points could be sound files
  - If the task is image tagging, they could be picture files
- Examples of the expected output
  - In a speech-recognition task, these could be transcripts of sound files
  - In an image task, expected outputs could tags such as "dog", "cat", and so on
- A way to measure whether the algorithm is doing a good job
  - This is needed to determine the distance between the output and its expected output. 
  - The measurement is used as a feedback signal to adjust the way the algorithm works.

::: r-stack
![](images/DLiR/mse-1.png){.fragment width="450"}

![](images/DLiR/mse-2.png){.fragment width="450"}

![](images/DLiR/mse-3.png){.fragment width="450"}

![](images/DLiR/mse-4.png){.fragment width="450"}
:::

## Anatomy of a Neural Network {auto-animate="true"}
- The [input data]{.text-red} and corresponding [targets]{.text-red}

::: r-stack
![](images/DLiR/nn-1.png){.fragment width="600"}
:::

## Anatomy of a Neural Network {auto-animate="true"}
- The [input data]{.text-red} and corresponding [targets]{.text-red}
- [Layers]{.text-red}, which are combined into a [network (or model)]{.text-red}

::: r-stack
![](images/DLiR/nn-1.png){.fragment width="600"}

![](images/DLiR/nn-2.png){.fragment width="600"}
:::

## Anatomy of a Neural Network {auto-animate="true"}
- The [input data]{.text-red} and corresponding [targets]{.text-red}
- [Layers]{.text-red}, which are combined into a [network (or model)]{.text-red}
- The [loss function]{.text-red}, which provides feedback for learning

::: r-stack
![](images/DLiR/nn-1.png){.fragment width="600"}

![](images/DLiR/nn-2.png){.fragment width="600"}

![](images/DLiR/nn-3.png){.fragment width="600"}
:::

## Anatomy of a Neural Network {auto-animate="true"}
- The [input data]{.text-red} and corresponding [targets]{.text-red}
- [Layers]{.text-red}, which are combined into a [network (or model)]{.text-red}
- The [loss function]{.text-red}, which provides feedback for learning
- The [optimizer]{.text-red}, which determines how learning proceeds

::: r-stack
![](images/DLiR/nn-1.png){.fragment width="600"}

![](images/DLiR/nn-2.png){.fragment width="600"}

![](images/DLiR/nn-3.png){.fragment width="600"}

![](images/DLiR/nn-4.png){.fragment width="600"}
:::

## LeNet-5: a pioneering 7-level CNN {.smaller}
::: {.incremental}
- The first successful practical application of neural nets came in 1989 from Bell Labs, when [Yann LeCun]{.text-red} combined the earlier ideas of convolutional neural networks and backpropagation, and applied them to the problem of classifying handwritten digits. 

- The resulting network, dubbed LeNet, was used by the USPS in the 1990s to automate the reading of ZIP codes on mail envelopes.

- LeNet-5 was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.
:::

<center>![](images/DLiR/cnn.gif){width="600"}</center>

# 2. Fundamentals of Neural Networks

-   **Perceptron**: basic neuron → activation(input \* weight + bias)

-   **Layers**: Dense layers, activation functions (`relu`, `sigmoid`, `softmax`)

-   **Forward & backward pass**: feedforward, gradient descent

::: shrink-code
``` r
model <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 10, activation = 'softmax')
```
:::

??? note Visualize network architecture with `plot_model()`.

# 3. Multilayer Perceptrons (MLPs)

## Classification on MNIST

``` r
model %>% compile(
  optimizer = 'rmsprop',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)
history <- model %>% fit(
  x_train, y_train,
  epochs = 10, batch_size = 128,
  validation_split = 0.2
)
```

-   **Data preprocessing**: normalization, one-hot encoding

-   **Callbacks**: EarlyStopping, ModelCheckpoint

??? note Show training curves: `plot(history)`.

# 4. Convolutional Neural Networks (CNNs)

-   **Concepts**: filters, pooling, feature maps

-   **Example**:

``` r
cnn <- keras_model_sequential() %>%
  layer_conv_2d(filters=32, kernel_size=c(3,3), activation='relu', input_shape=c(28,28,1)) %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), activation='relu') %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>%
  layer_flatten() %>%
  layer_dense(units=64, activation='relu') %>%
  layer_dense(units=10, activation='softmax')
```

-   **Image augmentation**: `image_data_generator()`

??? note Demo augmenting and fitting on CIFAR-10 subset.

# 5. Recurrent Neural Networks & LSTM

-   **RNN basics**: sequence data, time steps

-   **LSTM**: handling long-term dependencies

``` r
rnn <- keras_model_sequential() %>%
  layer_lstm(units=128, input_shape=c(timesteps, features)) %>%
  layer_dense(units=1, activation='sigmoid')
```

-   **Use case**: sentiment analysis, text generation

??? note Show example on IMDB dataset: `dataset_imdb()`.

# 6. Autoencoders & Unsupervised Learning

-   **Architecture**: encoder ⇄ bottleneck ⇄ decoder

``` r
ae <- keras_model_sequential() %>%
  layer_dense(units=64, activation='relu', input_shape=c(784)) %>%
  layer_dense(units=32, activation='relu') %>%
  layer_dense(units=64, activation='relu') %>%
  layer_dense(units=784, activation='sigmoid')
```

-   **Applications**: dimensionality reduction, denoising

??? note Demo training on noisy MNIST.

# 7. Model Evaluation & Hyperparameter Tuning

-   **Metrics**: accuracy, precision/recall, ROC-AUC

-   **Plots**: confusion matrix, learning curves

-   **Tuning**: `tfruns`, `keras_tuner`

-   **K-fold CV**: manual splits or `rsample`

??? note Illustrate hyperband tuning example.

# 8. Advanced Topics & Deployment

-   **Transfer Learning**: `application_resnet50()`, fine-tuning

-   **Custom layers & callbacks**

-   **Model saving/loading**: `save_model_hdf5()`, `load_model_hdf5()`

-   **Deployment**: Plumber API, TensorFlow Serving, Shiny integration

??? note Example: wrap model in Plumber endpoint.

# 9. Hands-on Exercises (30 min)

1.  Build and train a CNN on Fashion MNIST

2.  Implement an LSTM for sequence prediction (e.g., sine wave)

3.  Create an autoencoder for dimensionality reduction and visualize embeddings

??? note Encourage pair work and code sharing.

# Resources & Further Reading

-   Book: *Deep Learning with R* by Francois Chollet & J.J. Allaire

-   Keras docs: https://keras.rstudio.com/

-   TensorFlow for R: https://tensorflow.rstudio.com/

-   Tutorials: https://www.coursera.org/learn/deep-learning

??? note Provide links for reference.
